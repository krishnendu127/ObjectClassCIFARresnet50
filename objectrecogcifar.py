# -*- coding: utf-8 -*-
"""ObjectRecogCIFAR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vSZ2lPu3DQCbt4ia8nMm3xnAoMdjpQRA
"""

!pip install kaggle

#configuring the path of kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#dataset api
!kaggle competitions download -c cifar-10

!ls

#extracting the compresed dataset
from zipfile import ZipFile
dataset='/content/cifar-10.zip'
with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('the dataset is extracted')

!ls

!pip install py7zr

import py7zr
archive=py7zr.SevenZipFile( '/content/train.7z', mode='r')
archive.extractall()
archive.close()

!ls

"""importing the dependencies"""

import os
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

filenames=os.listdir('/content/train')

len(filenames)

"""Labels processing"""

labels_df=pd.read_csv('/content/trainLabels.csv')

labels_df.shape

labels_df.head()

labels_df[labels_df['id']==7796]

labels_df.head(10)

labels_df['label'].value_counts()

labels_dictionary={'airplane':0 , 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'ship': 7, 'truck': 8, 'horse':9}

labels=[labels_dictionary[i] for i in labels_df['label']]

print(labels[0:5])

#displaying sample image
import cv2
from google.colab.patches import cv2_imshow
img=cv2.imread('/content/train/7765.png')
cv2_imshow(img)

labels_df.head()

id_list=list(labels_df['id'])

print(id_list[0:5])

"""Image Processing"""

#convert images to numpy arrays

train_data_folder='/content/train/'

data=[]
for id in id_list:
  image=Image.open(train_data_folder + str(id) + '.png')
  image=np.array(image)
  data.append(image)

type(data)

len(data)

type(data[0])

data[0].shape

#convert images and labels to numpy arrays

x=np.array(data)
y=np.array(labels)

type(x)

print(x.shape)
print(y.shape)

"""train test split"""

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=55)

print(x.shape, x_train.shape)

"""Data Scaling"""

x_train_scaled=x_train/255
x_test_scaled=x_test/255

print(x_train_scaled[0])

"""Building a neural netwrok"""

import tensorflow as tf
from tensorflow import keras

num_of_classes=10
#setting up the layers of neural network

model= keras.Sequential([
    keras.layers.Flatten(input_shape=(32,32,3)),
    keras.layers.Dense(80,activation='relu'),
    keras.layers.Dense(num_of_classes,activation='softmax')
])

#compiling the netork

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

#training the model

model.fit(x_train_scaled,y_train, validation_split=0.1, epochs=10)

"""Now, Using ResNet 50"""

from tensorflow.keras import Sequential,models,layers
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras import optimizers

convolutional_base= ResNet50(weights='imagenet', include_top=False, input_shape=(256,256,3))

convolutional_base.summary()

model = models.Sequential()
model.add(layers.UpSampling2D((2,2)))
model.add(layers.UpSampling2D((2,2)))
model.add(layers.UpSampling2D((2,2)))
model.add(convolutional_base)
model.add(layers.Flatten())
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
model.add(layers.Dense(num_of_classes, activation='softmax'))

#compiling the model
model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['acc'])

history= model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=10)

loss,accuracy=model.evaluate(x_test_scaled, y_test)
print('test accuracy=',accuracy)